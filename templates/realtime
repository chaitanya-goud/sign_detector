<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Real-Time Hand Sign Detection</title>
    <style>
        body { font-family: Arial, sans-serif; background: #f4f4f4; text-align: center; }
        #video { border: 2px solid #333; border-radius: 8px; margin-top: 20px; }
        #prediction { font-size: 2em; margin-top: 20px; color: #007bff; }
        .container { margin-top: 40px; }
        button { padding: 10px 20px; font-size: 1em; margin-top: 20px; }
    </style>
    <!-- Load TensorFlow.js and HandPose model -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/handpose"></script>
</head>
<body>
    <div class="container">
        <h1>Real-Time Hand Sign Detection</h1>
        <video id="video" width="480" height="360" autoplay></video>
        <div id="prediction">Prediction: ...</div>
        <button onclick="toggleStream()" id="toggleBtn">Stop</button>
    </div>
    <script>
        const video = document.getElementById('video');
        const predictionDiv = document.getElementById('prediction');
        const toggleBtn = document.getElementById('toggleBtn');
        let streaming = true;
        let stream = null;
        let intervalId = null;
        let model = null;

        async function startWebcam() {
            stream = await navigator.mediaDevices.getUserMedia({ video: true });
            video.srcObject = stream;
            video.play();
            streaming = true;
            toggleBtn.textContent = 'Stop';
            if (!model) {
                predictionDiv.textContent = 'Loading handpose model...';
                model = await handpose.load();
                predictionDiv.textContent = 'Model loaded!';
            }
            intervalId = setInterval(captureAndSendLandmarks, 300);
        }

        function stopWebcam() {
            if (stream) {
                stream.getTracks().forEach(track => track.stop());
            }
            streaming = false;
            toggleBtn.textContent = 'Start';
            clearInterval(intervalId);
        }

        function toggleStream() {
            if (streaming) {
                stopWebcam();
            } else {
                startWebcam();
            }
        }

        async function captureAndSendLandmarks() {
            if (!streaming || !model) return;
            const predictions = await model.estimateHands(video, true);
            if (predictions.length > 0) {
                // Hand landmarks: 21 points, each with x, y, z
                const landmarks = predictions[0].landmarks.flat();
                try {
                    const response = await fetch('/predict_landmarks', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({ landmarks: landmarks })
                    });
                    const result = await response.json();
                    predictionDiv.textContent = 'Prediction: ' + (result.prediction || '...');
                } catch (e) {
                    predictionDiv.textContent = 'Prediction: Error';
                }
            } else {
                predictionDiv.textContent = 'Prediction: No hand detected';
            }
        }

        window.onload = startWebcam;
    </script>
</body>
</html> 
